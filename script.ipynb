{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99128a-ee54-4c7c-a638-d23f274d9019",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from flask import Flask, request, jsonify\n",
    "from hydra import compose, initialize\n",
    "from pathlib import Path\n",
    "from omegaconf import OmegaConf\n",
    "from yolo.config.config import Config\n",
    "from yolo.model.yolo import YOLO, create_model\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Config path\n",
    "config_path = \"../yolo/config\"  # Make sure this is relative\n",
    "config_name = \"config\"\n",
    "\n",
    "model_cache = {}\n",
    "\n",
    "models = []\n",
    "\n",
    "# Project root setup\n",
    "project_root = Path(__file__).resolve().parent.parent\n",
    "sys.path.append(str(project_root))\n",
    "print(project_root)\n",
    "\n",
    "# ----------- TEST FUNCTIONS -----------\n",
    "\n",
    "def test_build_model_v9c():\n",
    "    with initialize(config_path=config_path, version_base=None):\n",
    "        cfg: Config = compose(config_name=config_name)\n",
    "        OmegaConf.set_struct(cfg.model, False)\n",
    "        cfg.weight = None\n",
    "        model = YOLO(cfg.model)\n",
    "        assert len(model.model) == 39\n",
    "    print(\"✅ test_build_model_v9c passed\")\n",
    "\n",
    "\n",
    "def test_build_model_v9m():\n",
    "    with initialize(config_path=config_path, version_base=None):\n",
    "        cfg: Config = compose(config_name=config_name, overrides=[\"model=v9-m\"])\n",
    "        OmegaConf.set_struct(cfg.model, False)\n",
    "        cfg.weight = None\n",
    "        model = YOLO(cfg.model)\n",
    "        assert len(model.model) == 39\n",
    "    print(\"✅ test_build_model_v9m passed\")\n",
    "\n",
    "\n",
    "def test_build_model_v7():\n",
    "    with initialize(config_path=config_path, version_base=None):\n",
    "        cfg: Config = compose(config_name=config_name, overrides=[\"model=v7\"])\n",
    "        OmegaConf.set_struct(cfg.model, False)\n",
    "        cfg.weight = None\n",
    "        model = YOLO(cfg.model)\n",
    "        assert len(model.model) == 106\n",
    "    print(\"✅ test_build_model_v7 passed\")\n",
    "\n",
    "\n",
    "def get_cfg() -> Config:\n",
    "    with initialize(config_path=config_path, version_base=None):\n",
    "        cfg: Config = compose(config_name=config_name)\n",
    "        cfg.weight = None\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_model(cfg: Config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = create_model(cfg.model, weight_path=None)\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def test_model_basic_status(model):\n",
    "    assert isinstance(model, YOLO)\n",
    "    assert len(model.model) == 39\n",
    "    print(\"✅ test_model_basic_status passed\")\n",
    "\n",
    "\n",
    "def test_yolo_forward_output_shape(model):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    dummy_input = torch.rand(2, 3, 640, 640, device=device)\n",
    "\n",
    "    output = model(dummy_input)\n",
    "    output_shape = [(cls.shape, anc.shape, box.shape) for cls, anc, box in output[\"Main\"]]\n",
    "    assert output_shape == [\n",
    "        (torch.Size([2, 80, 80, 80]), torch.Size([2, 16, 4, 80, 80]), torch.Size([2, 4, 80, 80])),\n",
    "        (torch.Size([2, 80, 40, 40]), torch.Size([2, 16, 4, 40, 40]), torch.Size([2, 4, 40, 40])),\n",
    "        (torch.Size([2, 80, 20, 20]), torch.Size([2, 16, 4, 20, 20]), torch.Size([2, 4, 20, 20])),\n",
    "    ]\n",
    "    print(\"✅ test_yolo_forward_output_shape passed\")\n",
    "\n",
    "def load_model(model_type: str, config: Config):\n",
    "    if model_type == \"v9c\":\n",
    "        return YOLO(config.model)  # Load model v9c\n",
    "    elif model_type == \"v9m\":\n",
    "        config.model = \"v9-m\"  # Override config for v9-m\n",
    "        return YOLO(config.model)  # Load model v9m\n",
    "    elif model_type == \"v7\":\n",
    "        config.model = \"v7\"  # Override config for v7\n",
    "        return YOLO(config.model)  # Load model v7\n",
    "    else:\n",
    "        raise ValueError(\"Unknown model type!\")\n",
    "\n",
    "import torch\n",
    "\n",
    "def unload_model(model):\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "def save_model_to_disk(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "def load_model_from_disk(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n",
    "\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "\n",
    "def checkpoint_forward(model, input_data):\n",
    "    return checkpoint.checkpoint(model, input_data)\n",
    "\n",
    "\n",
    "def run_inference(model, input_data):\n",
    "    \"\"\"\n",
    "    Run inference on a single model.\n",
    "    \n",
    "    Args:\n",
    "        model: The model to run inference on.\n",
    "        input_data: The input data for the model.\n",
    "    \n",
    "    Returns:\n",
    "        The output of the model.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():  # Disable gradient calculation for inference\n",
    "        output = model(input_data)\n",
    "\n",
    "    return [[ k.tolist() for k in w] for w in output[\"Main\"]]\n",
    "\n",
    "def balance_workload(models, input_data_list):\n",
    "    \"\"\"\n",
    "    Dynamically balance workload across multiple models running in parallel by checking if a model is free.\n",
    "    \n",
    "    Args:\n",
    "        models: A list of models to run inference on.\n",
    "        input_data_list: A list of input data corresponding to each model.\n",
    "    \n",
    "    Returns:\n",
    "        A list of outputs from each model.\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    future_to_model = {}\n",
    "    # Create a status dictionary to track if models are free or busy\n",
    "    model_status = {model: \"free\" for model in models}  # 'free' or 'busy'\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "        # Track the index for the next available task\n",
    "        task_index = 0\n",
    "        \n",
    "        while task_index < len(input_data_list):\n",
    "            # Look for free models and assign them a task\n",
    "            for model in models:\n",
    "                if model_status[model] == \"free\" and task_index < len(input_data_list):\n",
    "                    input_data = input_data_list[task_index]\n",
    "                    # print(input_data.shape)\n",
    "                    # print(input_data.shape)\n",
    "                    future = executor.submit(run_inference, model, input_data)\n",
    "                    future_to_model[future] = model\n",
    "                    model_status[model] = \"busy\"  # Mark model as busy\n",
    "                    task_index += 1  # Move to the next task\n",
    "                    break  # Only start a task for one free model at a time\n",
    "\n",
    "            # Collect results as they complete and assign tasks to free models\n",
    "            for future in as_completed(future_to_model):\n",
    "                model = future_to_model[future]\n",
    "                try:\n",
    "                    output = future.result()\n",
    "                    outputs.append(output)\n",
    "                    \n",
    "                    print(f\"{model_status[model]}\")\n",
    "                    # After a model finishes, mark it as free\n",
    "                    model_status[model] = \"free\"\n",
    "                    # print(f\"Completed task {len(outputs)} / {len(input_data_list)}\")\n",
    "                    # print(f\"Next task index: {task_index}\")\n",
    "                    \n",
    "                    # Free memory from completed task\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                    # Clear unnecessary memory used by output and intermediate variables\n",
    "                    del output  # Delete the output of the model after it's added to results\n",
    "\n",
    "                    # Assign a new task to the now-free model if there are remaining tasks\n",
    "                    if task_index < len(input_data_list):\n",
    "                        new_input_data = input_data_list[task_index]\n",
    "                        future_to_model[executor.submit(run_inference, model, new_input_data)] = model\n",
    "                        model_status[model] = \"busy\"  # Mark model as busy again\n",
    "                        task_index += 1  # Move to the next task\n",
    "                    else:\n",
    "                        pass\n",
    "                        # print(\"All tasks completed!\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Model {model} generated an exception: {e}\")\n",
    "                    del future_to_model[future]  # Clean up the future in case of error\n",
    "    \n",
    "    # Free any remaining memory after all tasks are completed\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "\n",
    "def prepare_models_and_inputs(device):\n",
    "    \"\"\"\n",
    "    Prepare models and input data for inference.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing a list of models and a list of input data.\n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    # Load multiple models (e.g., v9c, v9m, v7)\n",
    "    model_v9c = get_model(cfg).to(device)\n",
    "    \n",
    "    models = [model_v9c for w in range(8)]\n",
    "    \n",
    "    # Prepare input data for each model\n",
    "    input_data_list = [torch.rand(2, 3, 640, 640, device=device) for i in range(2)]  # Input for v9c\n",
    "    \n",
    "    return models, input_data_list\n",
    "\n",
    "\n",
    "def prepare_models(device, num_models=8):\n",
    "    \"\"\"\n",
    "    Prepare models and input data for inference.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing a list of models and a list of input data.\n",
    "    \"\"\"\n",
    "    cfg = get_cfg()\n",
    "    \n",
    "    # Load multiple models (e.g., v9c, v9m, v7)\n",
    "    model_v9c = get_model(cfg).to(device)\n",
    "    \n",
    "    models = [model_v9c for w in range(num_models)]\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "def get_models():\n",
    "\n",
    "    return models\n",
    "\n",
    "\n",
    "@app.route('/infer', methods=['POST'])\n",
    "def infer():\n",
    "    \"\"\"\n",
    "    Endpoint to accept input for various models and run inference.\n",
    "    Expected input: JSON with a list of input data (e.g., images or tensors).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print('---------1')\n",
    "        # Get input data from request   \n",
    "        data = request.get_json()\n",
    "        if not data or 'inputs' not in data:\n",
    "            return jsonify({\"error\": \"No input data provided\"}), 400\n",
    "        \n",
    "        models = get_models()\n",
    "\n",
    "        print('---------2')\n",
    "\n",
    "        [print(np.array(w).shape) for w in data['inputs']]\n",
    "        # Convert input data to torch tensors\n",
    "        input_data_list = [torch.tensor(item, device=device).float() for item in data['inputs']]\n",
    "\n",
    "        input_data = torch.stack(input_data_list)  # Shape: (batch_size, height, width)\n",
    "\n",
    "        # Step 2: Add channel dimension (assuming single-channel input)\n",
    "        input_data = input_data.unsqueeze(1)  # Shape: (batch_size, channels=1, height, width)\n",
    "        input_data = input_data.squeeze(1)\n",
    "        input_data = input_data.squeeze(0)\n",
    "        input_data = input_data.unsqueeze(0)\n",
    "        input_data = input_data.unsqueeze(0)\n",
    "        # Final 4D tensor\n",
    "\n",
    "        print(input_data.shape)\n",
    "        # print(input_data_list[0].shape)\n",
    "        print('---------3')\n",
    "        # Balance workload and run inference in parallel\n",
    "        outputs = balance_workload(models, input_data)\n",
    "        print('---------4')\n",
    "        # Format output for response\n",
    "        output_data = [{\"model\": i, \"output\": output} for i, output in enumerate(outputs)]\n",
    "        print('---------5')\n",
    "        return jsonify({\"outputs\": output_data})\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Welcome to the model inference API!\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    models = prepare_models(device)\n",
    "    print('MODEL')\n",
    "\n",
    "    port = int(os.getenv(\"PORT\", 5000))\n",
    "    print(port)\n",
    "    port += 1\n",
    "    app.run(debug=True, port=port)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a00905-4aca-486f-b4dc-7cc222ca2b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
